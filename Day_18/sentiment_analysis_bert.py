# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Bert (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Aj4EDJPh2Cg_l0gbvTY-cxtEmbH1tGvI
"""

!pip install -q kaggle
from google.colab import files
uploaded = files.upload()

! mkdir ~/.kaggle
 ! cp kaggle.json ~/.kaggle/
 ! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
!unzip '/content/imdb-dataset-of-50k-movie-reviews.zip'

!kaggle datasets download -d abhishek/bert-base-uncased
!unzip '/content/bert-base-uncased.zip'

!pip install transformers

import transformers
from tqdm import tqdm
import pandas as pd
import torch

df_main = pd.read_csv('/content/IMDB Dataset.csv')
df_main.head()

df_positive = df_main[df_main['sentiment'] == 'positive'].head(5000)
df_negative = df_main[df_main['sentiment'] == 'negative'].head(5000)
df_final = pd.concat([df_positive, df_negative])
df_final.to_csv('imdb_short.csv', index=None)

df = pd.read_csv('/content/imdb_short.csv')
df.head()

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN = 100
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 5
BERT_PATH= '/content/'
MODEL_PATH = '/content/model.bin'
TRAINING_FILE = '/content/imdb_short.csv'
TOKENIZER = transformers.BertTokenizer.from_pretrained(
    BERT_PATH, do_lower_case=True
)

import torch.nn as nn
import torch

class BERTBasedUncased(nn.Module):
  def __init__(self):
    super(BERTBasedUncased, self).__init__()
    self.bert = transformers.BertModel.from_pretrained(BERT_PATH)
    self.bert_drop = nn.Dropout(0.3)
    self.out = nn.Linear(768, 1)

  def forward(self, ids, mask, token_type_ids):
    _, o2 = self.bert(
        ids, 
        attention_mask = mask,
        token_type_ids = token_type_ids, return_dict=False
    )
    #print("hello",o2)
    bo = self.bert_drop(o2)
    output = self.out(bo)
    return output

class BERTDataset:
  def __init__(self, review, target):
    self.review = review
    self.target = target
    self.tokenizer = TOKENIZER
    self.max_len = MAX_LEN
    
  def __len__(self):
    return len(self.review)
  
  def __getitem__(self, item):
    review = str(self.review[item])
    review = " ".join(review.split())

    inputs = self.tokenizer.encode_plus(
        review,
        None,
        add_special_tokens=True,
        max_length = self.max_len,
        truncation=True
    )
    ids = inputs["input_ids"]
    mask = inputs["attention_mask"]
    token_type_ids = inputs["token_type_ids"]

    padding_length = self.max_len - len(ids)
    ids = ids + ([0] * padding_length)
    mask = mask + ([0] * padding_length)
    token_type_ids = token_type_ids + ([0] * padding_length)

    return {
        'ids': torch.tensor(ids, dtype=torch.long),
        'mask': torch.tensor(mask, dtype=torch.long),
        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
        'target': torch.tensor(self.target[item], dtype=torch.float)
    }

def loss_fn(outputs, targets):
  return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))

def train_fn(data_loader, model, optimizer, device, scheduler):
  model.train()

  for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
    ids = d["ids"]
    token_type_ids = d["token_type_ids"]
    mask = d["mask"]
    targets = d["target"]

    ids = ids.to(device, dtype=torch.long)
    token_type_ids = token_type_ids.to(device, dtype=torch.long)
    mask = mask.to(device, dtype=torch.long)
    targets = targets.to(device, dtype=torch.float)

    optimizer.zero_grad()
    outputs = model(
        ids = ids,
        mask = mask,
        token_type_ids = token_type_ids
    )

    loss = loss_fn(outputs, targets)
    loss.backward()
    optimizer.step()
    scheduler.step()

def eval_fn(data_loader, model, device):
  model.eval()
  fin_targets = []
  fin_outputs = []
  with torch.no_grad():
    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
      ids = d['ids']
      token_type_ids = d['token_type_ids']
      mask = d['mask']
      targets = d['target']

      ids = ids.to(device, dtype=torch.long)
      token_type_ids = token_type_ids.to(device, dtype=torch.long)
      mask = mask.to(device, dtype=torch.long)
      targets = targets.to(device, dtype=torch.float)

      outputs = model(
          ids = ids,
          mask = mask,
          token_type_ids = token_type_ids
      )
      fin_targets.extend(targets.cpu().detach().numpy().tolist())
      fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
  return fin_outputs, fin_targets

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
from transformers import AdamW, get_linear_schedule_with_warmup

def run():
  df = pd.read_csv(TRAINING_FILE).fillna("none")
  df.sentiment = df.sentiment.apply(
      lambda x: 1 if x == "positive" else 0
  )
  df_train, df_valid = train_test_split(
      df,
      test_size = 0.1,
      random_state=42,
      stratify=df.sentiment.values
      )
  
  df_train = df_train.reset_index(drop=True)
  df_valid = df_valid.reset_index(drop=True)

  train_dataset = BERTDataset(
      review=df_train.review.values,
      target = df_train.sentiment.values
  )
  train_data_loader = torch.utils.data.DataLoader(
      train_dataset,
      batch_size=TRAIN_BATCH_SIZE,
      num_workers=4
  )
  valid_dataset = BERTDataset(
      review=df_valid.review.values,
      target = df_valid.sentiment.values
      ) 
  valid_data_loader = torch.utils.data.DataLoader(
      valid_dataset,
      batch_size=VALID_BATCH_SIZE,
      num_workers=4
  )
  device = torch.device(DEVICE)
  model = BERTBasedUncased()
  model.to(device)
  
  param_optimizer = list(model.named_parameters())
  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
  optimizer_parameters = [
    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay':0.001},
    {'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0},
  ]

  num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)
  optimizer = AdamW(optimizer_parameters, lr=3e-5)
  scheduler = get_linear_schedule_with_warmup(
      optimizer,
      num_warmup_steps=0,
      num_training_steps = num_train_steps
  )

  

  best_accuracy = 0
  for epoch in range(EPOCHS):
    train_fn(train_data_loader, model, optimizer, device, scheduler)
    outputs, targets = eval_fn(valid_data_loader, model, device)
    outputs = np.array(outputs) >= 0.5
    accuracy = metrics.accuracy_score(targets, outputs)
    print(f"Accuracy Score = {accuracy}")
    if accuracy > best_accuracy:
      torch.save(model.state_dict(), MODEL_PATH)
      best_accuracy = accuracy

def sentence_prediction(sentence, model): 
  tokenizer = TOKENIZER
  max_length = MAX_LEN
  review = str(sentence)
  review = " ".join(review.split())

  inputs = tokenizer.encode_plus(
        review,
        None,
        add_special_tokens=True,
        max_length = MAX_LEN,
        truncation=True
    )
  ids = inputs["input_ids"]
  mask = inputs["attention_mask"]
  token_type_ids = inputs["token_type_ids"]

  padding_length = MAX_LEN - len(ids)
  ids = ids + ([0] * padding_length)
  mask = mask + ([0] * padding_length)
  token_type_ids = token_type_ids + ([0] * padding_length)

  ids =  torch.tensor(ids, dtype=torch.long).unsqueeze(0)
  mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)
  token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)

  ids = ids.to(DEVICE, dtype=torch.long)
  token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)
  mask = mask.to(DEVICE, dtype=torch.long)
  #targets = targets.to(DEVICE, dtype=torch.float)

  outputs = model(
      ids = ids,
      mask = mask,
      token_type_ids = token_type_ids
  )

  outputs = torch.sigmoid(outputs).cpu().detach().numpy()
  return outputs[0][0]

if __name__=='__main__': 
  run()
  model = BERTBasedUncased()
  model.load_state_dict(torch.load(MODEL_PATH))
  model.to(DEVICE)
  sentence = "The weather is nice today"
  prediction = sentence_prediction(sentence, model)
  print(prediction)
  sentence = "The weather is bad"
  prediction = sentence_prediction(sentence, model)
  print(prediction)


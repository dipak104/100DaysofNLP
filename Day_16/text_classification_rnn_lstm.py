# -*- coding: utf-8 -*-
"""Text_Classification_RNN_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Gt58qURTKUEbQlyP-_34EBzI0uT_MR0
"""

#!pip install torch==1.6.0 torchvision==0.7.0

import torch
print(torch.__version__)
from torchtext import data

SEED = 1111
torch.manual_seed(SEED)
TEXT = data.Field(tokenize='spacy', include_lengths=True) # If spacy not passed then it will split the text on the basis of spaces.
LABEL = data.LabelField(dtype=torch.float)

"""Downloading IMDB dataset from torchtext"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from torchtext import datasets
# 
# train, test = datasets.IMDB.splits(TEXT, LABEL)

len(train), len(test)

print(vars(train.examples[0]))

"""# We only have train and test data in IMDB dataset
Let's create validation data out of train data

"""

# The following code automatically downloads the IMDb dataset and splits it into the 
# canonical train/test splits as torchtext.datasets objects.
import random
train, valid = train.split(random_state = random.seed(SEED))

"""Download the word embeddings - "glove.6B.100d". The reason to use pre_trained word embeddings is they are initialized with pre-trained vectors. These pre-trained vectors already have words with similar semantic meaning close together in vector space. This gives our embeding layer a good initialization as it doen not have to learn these relations from scratch."""

#!wget http://nlp.stanford.edu/data/glove.6B.zip

#!unzip '/content/glove.6B.zip'

#import torchtext.vocab as vocab

#glove = vocab.GloVe(name='6B', dim=100)
#print('Loaded {} words'.format(len(glove.itos)))

# We have to build a vocabulary. A look up table where every unique word is mapped to a integer
MAX_VOCAB_SIZE = 25_000

TEXT.build_vocab(train, 
                 max_size=MAX_VOCAB_SIZE,
                 vectors = "glove.6B.100d",
                 unk_init = torch.Tensor.normal_)
LABEL.build_vocab(train)

len(TEXT.vocab), len(LABEL.vocab)
# The two additional tokens in TEXT.vocab is <unk>, <pad>

#vars(TEXT.vocab)

vars(LABEL.vocab)

# Most common words in the voabulary with frequencies
TEXT.vocab.freqs.most_common(10)

# To check the vocabulary
TEXT.vocab.itos[:10]     #itos - integer to string

# Check the labels    # stor - string to integer
LABEL.vocab.stoi

"""Final step is to create the iterators.
We'll use a BucketIterator which is a special type of
iterator that will return a batch of examples where each
example is of a similar length, minimizing the amount of 
padding per example.
"""

BATCH_SIZE = 64
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iter, valid_iter, test_iter = data.BucketIterator.splits(
    (train, valid, test),
    batch_size = BATCH_SIZE,
    sort_within_batch=True,
    device = device)

# Build the RNN Model

import torch.nn as nn

class RNN(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):
    super().__init__()
    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional=bidirectional, dropout=dropout)
    self.linear = nn.Linear(hidden_dim*2, output_dim)
    self.dropout = nn.Dropout(dropout)
  def forward(self, text, text_length):
    embedded = self.dropout(self.embedding(text))
    pack_embedding = nn.utils.rnn.pack_padded_sequence(embedded, text_length.cpu())
    pack_output, (hidden, cell) = self.rnn(pack_embedding)
    output, output_length = nn.utils.rnn.pad_packed_sequence(pack_output)
    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))
    return self.linear(hidden)

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

model = RNN(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM,
            N_LAYERS,
            BIDIRECTIONAL,
            DROPOUT,
            PAD_IDX)

embeddings = TEXT.vocab.vectors
embeddings.shape

model.embedding.weight.data.copy_(embeddings)

UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]
model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

model.embedding.weight.data

import torch.optim as optim
optimizer = optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss()

model = model.to(device)
criterion = criterion.to(device)

def model_accuracy(predictions, y):
  pred = torch.round(torch.sigmoid(predictions))
  actual = (pred == y).float()
  acc = actual.sum() / len(actual)
  return acc

def train(model, iterator, optimizer, criterion):
  epoch_loss = 0
  epoch_acc = 0
  model.train()
  for batch in iterator:
    optimizer.zero_grad()    # zero the gradients
    #print(batch.text)
    text, text_length = batch.text
    #print("*")
    predictions = model(text, text_length).squeeze(1)
    #print("*")
    loss = criterion(predictions, batch.label.float())   # Calculate the loss
    acc = model_accuracy(predictions, batch.label)
    loss.backward()  # calculate the gradient of each parameter with loss.backward()
    optimizer.step() # update the parameters using the gradients and optimizer algorithm
    epoch_loss += loss.item()
    epoch_acc += acc.item()
        
  return epoch_loss / len(iterator), epoch_acc / len(iterator)

"""In eval function we do not want to update the parameters when evaluating.
So, we don't need optimizer.zero_grad(), loss.backward() and optimizer.step().
"""

def eval(model, iterator, criterion):
  epoch_loss = 0
  epoch_acc = 0
  model.eval()

  """No gradients are calculated on PyTorch operations inside the with 
  no_grad() block. This causes less memory to be 
  used and speeds up computation"""
  with torch.no_grad(): 
    for batch in iterator:
      text, text_length = batch.text
      predictions = model(text, text_length).squeeze(1)
      loss = criterion(predictions, batch.label)
      acc = model_accuracy(predictions, batch.label)

      epoch_loss += loss.item()
      epoch_acc += acc.item()
  return epoch_loss / len(iterator), epoch_acc / len(iterator)

import time
EPOCHS = 5
opt_valid_loss = float('inf')

for epoch in range(EPOCHS):
  start_time = time.time()
  #print(start_time)
  train_loss, train_acc = train(model, train_iter, optimizer, criterion)
  print("*")
  valid_loss, valid_acc = eval(model, valid_iter, criterion)
  print("*")
  end_time = time.time()
  if valid_loss < opt_valid_loss:
    opt_valid_loss = valid_loss
    torch.save(model.state_dict(), 'LSTM-RNN-model.pt')
    
    print(f'Epoch: {epoch+1}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')

"""As we can see that the accuracy is poor. So, we need to improve the model by hypertuning it or to use different Neural Network."""

model.load_state_dict(torch.load('/content/LSTM-RNN-model.pt'))

test_loss, test_acc = eval(model, test_iter, criterion)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')

import spacy
nlp = spacy.load('en')

def predict(model, text):
  model.eval()
  tokenize = [token.text for token in nlp.tokenizer(text)]
  index = [TEXT.vocab.stoi[t] for t in tokenize]
  length = [len(index)]
  tensor = torch.LongTensor(index).to(device).unsqueeze(1)
  len_tensor = torch.LongTensor(length)
  pred = torch.sigmoid(model(tensor, len_tensor))
  return pred.item()

predict(model, "This person is bad")

predict(model, "I am lovable")


{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_29.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMqz5qryyMhD"
      },
      "source": [
        "# Train language model from scratch using Transformers and Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrqw14zJycLG"
      },
      "source": [
        "Weâ€™ll use the Esperanto portion of the OSCAR corpus from INRIA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6uqOtEmsq5F",
        "outputId": "3ec08511-5a82-4043-c72e-341b988caa82"
      },
      "source": [
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-16 17:40:02--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 13.225.103.105, 13.225.103.115, 13.225.103.85, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|13.225.103.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXPMzgMowRnh",
        "outputId": "a9475c5b-dcd9-411a-a22e-61664857e7bf"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\r\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-h0vkuc94\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-h0vkuc94\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.4.0.dev0 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp36-none-any.whl size=1824852 sha256=4249f53c1ef6666919db9cdc8119f0a463890e87f597b44b5a3100f427296ead\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a78rq4p1/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "tokenizers                    0.10.1         \n",
            "transformers                  4.4.0.dev0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_iAsiCCyplQ"
      },
      "source": [
        "We are training a byte-level Byte-pair encoding tokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWE84Zx3wgrD"
      },
      "source": [
        "from pathlib import Path\r\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdO_hmrOwsD0"
      },
      "source": [
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD76-D6nwzTU",
        "outputId": "c57b9c3e-2376-48c0-9db0-4678b4ad89bc"
      },
      "source": [
        "paths"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oscar.eo.txt', 'oscarBERT/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hEuKr_Pw0Ac"
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o91jI0HIw3au"
      },
      "source": [
        "%%time\r\n",
        "tokenizer.train(files=paths, vocab_size = 20000, min_frequency=2, special_tokens=[\r\n",
        "      \"<s>\",\r\n",
        "      \"<pad>\",\r\n",
        "      \"</s>\",\r\n",
        "      \"<unk>\",\r\n",
        "      \"<mask>\",                                                                         \r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWspKASyxLyi"
      },
      "source": [
        "!mkdir oscarBERT\r\n",
        "tokenizer.save_model(\"oscarBERT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf9rfs54x3a7"
      },
      "source": [
        "vocab.json: List of the most frequent tokens ranked by frequency.\r\n",
        "merges.txt : list of merges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dImCc6rgx0Fj"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\r\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdxRlUKGx0BG"
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\r\n",
        "    \"./oscarBERT/vocab.json\",\r\n",
        "    \"./oscarBERT/merges.txt\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oybsS76Jxz-0"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\r\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\r\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnibe_8Axz63"
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvdlzswvxz2Q"
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\").tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWYFQSzxzvx"
      },
      "source": [
        "import torch\r\n",
        "from transformers import RobertaConfig\r\n",
        "from transformers import RobertaTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmjZwSL70dMT"
      },
      "source": [
        "config = RobertaConfig(\r\n",
        "    vocab_size = 20000,\r\n",
        "    max_position_embeddings = 514,\r\n",
        "    num_attention_heads = 12,\r\n",
        "    num_hidden_layers = 6,\r\n",
        "    type_vocab_size = 1,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNWoZUds0t8C"
      },
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained('./oscarBERT',max_len=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQpXngOC0-pl"
      },
      "source": [
        "Initialize our model only from Config as we are training it from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1CIOVCG07rF"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\r\n",
        "\r\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u15g1jkq1POU"
      },
      "source": [
        "model.num_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO7xW2Rs1SvQ"
      },
      "source": [
        "Build Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEtuSFzy1RQG"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "from transformers import LineByLineTextDataset\r\n",
        "\r\n",
        "dataset = LineByLineTextDataset(\r\n",
        "    tokenizer = tokenizer,\r\n",
        "    file_path = \"./oscar.eo.txt\",\r\n",
        "    block_size=64,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6euwFE31jGe"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\r\n",
        "\r\n",
        "data_collator = DataCollatorForLanguageModeling(\r\n",
        "    tokenzier = tokenizer, mlm=True, mlm_probability=0.20\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DTuAQX62HR8"
      },
      "source": [
        "Initialize the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtaV_IcP2GV2"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-95tVsM2GTk"
      },
      "source": [
        "training_args = TrainingArguments(\r\n",
        "    output_dir = './oscarBERT',\r\n",
        "    overwrite_output_dir = True,\r\n",
        "    num_train_epochs = 1,\r\n",
        "    per_gpu_train_batch_size = 64,\r\n",
        "    save_steps = 5000,\r\n",
        "    save_total_limit = 2,\r\n",
        ")\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model = model,\r\n",
        "    args = training_args,\r\n",
        "    data_collator = data_collator,\r\n",
        "    train_dataset = dataset,\r\n",
        "    prediction_loss_only = True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25W11JS72o-s"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HHUAV0z2oOv"
      },
      "source": [
        "%%time\r\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPmkKCaj2oMY"
      },
      "source": [
        "trainer.save_model(\"./oscarBERT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOA-8gCA24xB"
      },
      "source": [
        "Check the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ncsRUY2oIz"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPQRrP8R2oGh"
      },
      "source": [
        "fill_mask = pipeline(\r\n",
        "    \"fill-mask\",\r\n",
        "    model = \"./oscarBERT\",\r\n",
        "    tokenizer = \"./oscarBERT\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocLrkaV02oEL"
      },
      "source": [
        "fill_mask(\"Just lister <mask\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
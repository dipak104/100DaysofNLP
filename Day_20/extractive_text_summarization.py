# -*- coding: utf-8 -*-
"""Extractive Text Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFYOdaMm76JTIAeQQYpD2OUwWlXQfWA1
"""

import numpy as np
import pandas as pd
import nltk
import re
from nltk.stem import WordNetLemmatizer
from nltk import regexp_tokenize
from nltk.corpus import gutenberg 
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from sklearn.metrics.pairwise import cosine_similarity
import random

nltk.download('stopwords')
nltk.download('punkt') 
nltk.download('gutenberg')
nltk.download('wordnet')

#Get all books
book1 = nltk.corpus.gutenberg.raw('edgeworth-parents.txt')
book2 = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')
book3 = nltk.corpus.gutenberg.raw('carroll-alice.txt')
book4 = nltk.corpus.gutenberg.raw('chesterton-brown.txt')
book5 = nltk.corpus.gutenberg.raw('milton-paradise.txt')

books=[book1,book2,book3,book4,book5]
names=[]

#Get Labels
for book in books:
  add_names=re.findall("(\[[ a-zA-Z0-9 '_.+-|\]+ by [ a-zA-Z0-9-]+|\.[ a-zA-Z0-9-.]+\])", book)
  names.append(''.join(add_names[0]))

split_names = []
for a_name in names: 
  name = a_name.partition(" by ")[2].partition(']')[0]
  name = ''.join([i for i in name if not i.isdigit()])

  #split=re.split('by',a_name)    
  split_names.append(name)
  #print(split_names)

labels = []
#for new in split_names:
#  split_a = new[0]
 # split_b = re.sub(r'[\d+\\!"#$%&()*+,-./:;?@[\]^_`{|}~]','',split_a)
  #labels.append(split_b)

d = dict(zip(books, split_names))
df_final = pd.DataFrame(columns=['Text', 'Labels'])
df = pd.DataFrame(columns=['Text', 'Labels'])

  #Partition Data  
for text in books:
  doc = []
  final_text = clean_text(text).split()
  for i in range(0, 100):
    for j in range(0, 100):
      data_1 = (random.sample(final_text, j))
    doc.append(' '.join(str(''.join(str(x) for x in v)) for v in data_1))
  df['Text'] = doc
  df['Labels'] = d[str(text)]
  df_final = df_final.append(df, ignore_index=True)
       
       
print(df_final)

sentences = []
for s in df_final['Text']:
  sentences.append(sent_tokenize(s))

sentences = [y for x in sentences for y in x]

def remove_stopwords(sen):
    sen_new = " ".join([i for i in sen if i not in stop_words])
    return sen_new

sentences[:5]

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip '/content/glove*.zip'

word_embeddings = {}
f = open('/content/glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
f.close()

len(word_embeddings)

clean_sentences = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")

clean_sentences = [s.lower() for s in clean_sentences]

sentence_vectors = []
for i in clean_sentences:
  if len(i) != 0:
    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)
  else:
    v = np.zeros((100,))
  sentence_vectors.append(v)

sim_mat = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]

import networkx as nx

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

for i in range(10):
  print(ranked_sentences[i][1])






# -*- coding: utf-8 -*-
"""Text Classification_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aj78VqnS-JEsgONcTTOzJWDWsT-VPdos
"""

import torch
from torchtext import data
from torch.utils.data import Dataset

SEED = 1111
torch.manual_seed(SEED)
TEXT = data.Field(tokenize='spacy') # If spacy not passed then it will split the text on the basis of spaces.
LABEL = data.LabelField(dtype=torch.float)

"""Downloading IMDB dataset from torchtext"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from torchtext import datasets
# 
# train, test = datasets.IMDB.splits(TEXT, LABEL)

len(train), len(test)

print(vars(train.examples[0]))

"""# We only have train and test data in IMDB dataset
Let's create validation data out of train data

"""

# The following code automatically downloads the IMDb dataset and splits it into the 
# canonical train/test splits as torchtext.datasets objects.
import random
train, valid = train.split(random_state = random.seed(SEED))

# We have to build a vocabulary. A look up table where every unique word is mapped to a integer
MAX_VOCAB_SIZE = 25_000

TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)
LABEL.build_vocab(train)

len(TEXT.vocab), len(LABEL.vocab)
# The two additional tokens in TEXT.vocab is <unk>, <pad>

vars(LABEL.vocab)

# Most common words in the voabulary with frequencies
TEXT.vocab.freqs.most_common(10)

# To check the vocabulary
TEXT.vocab.itos[:10]     #itos - integer to string

# Check the labels    # stor - string to integer
LABEL.vocab.stoi

"""Final step is to create the iterators.
We'll use a BucketIterator which is a special type of
iterator that will return a batch of examples where each
example is of a similar length, minimizing the amount of 
padding per example.
"""

BATCH_SIZE = 64
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iter, valid_iter, test_iter = data.BucketIterator.splits(
    (train, valid, test),
    batch_size = BATCH_SIZE,
    device = device)

print(device)

train_iter

# Build the RNN Model

import torch.nn as nn

class RNN(nn.Module):
  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
    super().__init__()
    self.embedding = nn.Embedding(input_dim, embedding_dim)
    self.rnn = nn.RNN(embedding_dim, hidden_dim)
    self.linear = nn.Linear(hidden_dim, output_dim)
  def forward(self, text):
    embedded = self.embedding(text)
    output, hidden = self.rnn(embedded)
    assert torch.equal(output[-1, :, :], hidden.squeeze(0))
    return self.linear(hidden.squeeze(0))

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1

model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)

import torch.optim as optim
optimizer = optim.SGD(model.parameters(),lr=  1e-3)
criterion = nn.BCEWithLogitsLoss()

model = model.to(device)
criterion = criterion.to(device)

def model_accuracy(predictions, y):
  pred = torch.round(torch.sigmoid(predictions))
  actual = (pred == y).float()
  acc = actual.sum() / len(actual)
  return acc

def train(model, iterator, optimizer, criterion):
  epoch_loss = 0
  epoch_acc = 0
  model.train()
  for i, batch in enumerate(iterator):
    optimizer.zero_grad()    # zero the gradients
    #print(batch.text)
    predictions = model(batch.text).squeeze(1)
    loss = criterion(predictions, batch.label)   # Calculate the loss
    acc = model_accuracy(predictions, batch.label)
    loss.backward()  # calculate the gradient of each parameter with loss.backward()
    optimizer.step() # update the parameters using the gradients and optimizer algorithm
    epoch_loss += loss.item()
    epoch_acc += acc.item()
        
  return epoch_loss / len(iterator), epoch_acc / len(iterator)

"""In eval function we do not want to update the parameters when evaluating.
So, we don't need optimizer.zero_grad(), loss.backward() and optimizer.step().
"""

def eval(model, iterator, criterion):
  epoch_loss = 0
  epoch_acc = 0
  model.eval()

  with torch.no_grad(): """No gradients are calculated on PyTorch operations inside the with no_grad() block. This causes less memory to be used and speeds up computation"""
    for batch in iterator:
      predictions = model(batch.text).squeeze(1)
      loss = criterion(predictions, batch.label)
      acc = model_accuracy(predictions, batch.label)

      epoch_loss += loss.item()
      epoch_acc += acc.item()
  return epoch_loss / len(iterator), epoch_acc / len(iterator)

import time
EPOCHS = 10
opt_valid_loss = float('inf')

for epoch in range(EPOCHS):
  start_time = time.time()
  train_loss, train_acc = train(model, train_iter, optimizer, criterion)
  valid_loss, valid_acc = eval(model, valid_iter, criterion)
  end_time = time.time()
  if valid_loss < opt_valid_loss:
    opt_valid_loss = valid_loss
    torch.save(model.state_dict(), 'RNN-model.pt')
    
    print(f'Epoch: {epoch+1}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')

"""As we can see that the accuracy is poor. So, we need to improve the model by hypertuning it or to use different Neural Network."""

model.load_state_dict(torch.load('/content/RNN-model.pt'))

test_loss, test_acc = eval(model, test_iter, criterion)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')